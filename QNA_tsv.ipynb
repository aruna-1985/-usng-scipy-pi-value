{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QNA.tsv.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJ8sQ0S0X22ER/AQPvO5N6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruna20200/-usng-scipy-pi-value/blob/master/QNA_tsv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCAOzerRE24S"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import re\n",
        "import nltk \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqIJXoFJE8GS"
      },
      "source": [
        "#Loading Data\n",
        "path=\"/train.tsv\"\n",
        "df_data=pd.read_csv('/train.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMV9iD9aGMvY"
      },
      "source": [
        "df_data.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM0BKkR8GT7F"
      },
      "source": [
        "df_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIMKN_XrGa4-"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87DQ57zvHDRn"
      },
      "source": [
        "#Loading NLTK\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2DqhtBUHHQP"
      },
      "source": [
        "df_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zE67TGtKnng"
      },
      "source": [
        "df_data.Prompts.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy0ksRLfSDqq"
      },
      "source": [
        "Question_count=data.groupby('Question').count()\n",
        "plt.bar(Question_count.index.values, Question_count['Answer'])\n",
        "plt.xlabel('Question')\n",
        "plt.ylabel('Answer')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVy76xwyJej1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts= cv.fit_transform(data['Question'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIyszf4NPYss"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_counts, data['Answer'], test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i37tIBzsPn4f"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "predicted= clf.predict(X_test)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqhOxVjMPrUv"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "text_tf= tf.fit_transform(data['SuggestedQuestions'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tro7DKhQrVo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_tf, data['SuggestedQuestions'], test_size=0.3, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X0_p0xnQ03P"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "predicted= clf.predict(X_test)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwTtzB_RhkS"
      },
      "source": [
        "Question_count=data.groupby('Question').count()\n",
        "plt.bar(Question_count.index.values, Question_count['QnaId'])\n",
        "plt.xlabel('Question')\n",
        "plt.ylabel('QnaId')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-FeTX6Qfhat"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "predicted= clf.predict(X_test)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDgco81isKM7"
      },
      "source": [
        "!pip install glove_python\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download ('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ksxScosP_i"
      },
      "source": [
        "lines=  [\"i have few queries on vendors\" , \"what details are needed to ha\" , \"What is my name\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VELlinSKsaoh"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "word_tokens=[]\n",
        "i=0\n",
        "for line in lines:\n",
        " words = word_tokenize(line)\n",
        " word_tokens.insert(i,words)\n",
        " i=i+1\n",
        "print (word_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC5s60QTtZka"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "stop_words=stopwords.words('english') \n",
        "lines_without_stopwords=[] \n",
        "for line in lines: \n",
        "  stop_removed=[] \n",
        "for line in word_tokens:\n",
        "  for word in line: \n",
        "    if word not in stop_words: \n",
        "      stop_removed.append(word) \n",
        "print (stop_removed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXI2V2gUtZxO"
      },
      "source": [
        "from nltk import WordNetLemmatizer  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer() \n",
        "lines_with_lemmas=[] #stop words contain the set of stop words \n",
        "for line in lines: \n",
        "  lem_line=[] \n",
        "for word in stop_removed: \n",
        "  lem_line.append(wordnet_lemmatizer.lemmatize(word)) \n",
        "string='' \n",
        "new_lines=','.join([str(i) for i in lem_line])\n",
        "print (lem_line)\n",
        "print (new_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezkRluZFukSW"
      },
      "source": [
        "!pip install corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1h0Vfthu1WB"
      },
      "source": [
        "!apt install glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2NbLiZXuTVi"
      },
      "source": [
        "#importing the glove library\n",
        "from glove import Corpus, Glove\n",
        "# creating a corpus object\n",
        "corpus = Corpus() \n",
        "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "corpus.fit(new_lines, window=10)\n",
        "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "#We can set the learning rate as it uses Gradient Descent and number of components\n",
        "glove = Glove(no_components=5, learning_rate=0.05)\n",
        " \n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylk-x4vsuTaM"
      },
      "source": [
        "print (glove.word_vectors[glove.dictionary['SuggestedQuestions']])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}